{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_and_rnn._cmp.ipynb\"",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaroslavFYPM/RNN-vs-CNN/blob/main/RNNvsCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUI02UFXY5lZ"
      },
      "source": [
        "# Сравнение архитектуры RNN и CNN в задачах классификация текста.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eSlBJXbpNhH"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwNeXLCXr4ue"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBGF3mNQKAgN"
      },
      "source": [
        "SEED = 0xDEAD\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.random.manual_seed(SEED)\n",
        "torch.cuda.random.manual_seed_all(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-nNNuqZ9GN"
      },
      "source": [
        "Загрузим датасет новостей: `AgNews`. В нем разделены тексты на 4 темы: `World`, `Sports`, `Business`, `Sci/Tech`. Посмотрим на структуру датасета и на примеры текстов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LREB3dpscnH"
      },
      "source": [
        "dataset = datasets.load_dataset(\"ag_news\")\n",
        "dataset[\"train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YysP5HpsiBX"
      },
      "source": [
        "print(dataset[\"train\"][0])\n",
        "print(type(dataset[\"train\"][0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QK1tDYKaTJa"
      },
      "source": [
        "В `dataset` находятся `train` и `test` части датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjEB-Yv09AQo"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].info"
      ],
      "metadata": {
        "id": "-0UECxFM1KiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSIXBJPaegW"
      },
      "source": [
        "Чтобы превращать текст из набора слов в набор векторов мы будем использовать предобученные эмбеддинги. Посмотрим на их список и выберем один из них."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGW4jSiswVDy"
      },
      "source": [
        "print(\"\\n\".join(api.info()['models'].keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1RzPKhwC3q"
      },
      "source": [
        "word2vec = api.load(\"glove-twitter-50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAIznT0Kaue8"
      },
      "source": [
        "Токенезируем наш текст с помощью NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6k91UgcAQNz"
      },
      "source": [
        "MAX_LENGTH=128\n",
        "\n",
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda item: {\n",
        "        \"tokenized\": tokenizer.tokenize(item[\"text\"])[:MAX_LENGTH]\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqrtz1bDk2Ws"
      },
      "source": [
        "Создадим мапинг из токенов в индексы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMDWoNeEArA6"
      },
      "source": [
        "word2idx = {word: idx for idx, word in enumerate(word2vec.index2word)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_q_EgHlCWC"
      },
      "source": [
        "Переведем токены в индексы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVQX-b8mEKtA"
      },
      "source": [
        "def encode(word):\n",
        "    if word in word2idx.keys():\n",
        "        return word2idx[word]\n",
        "    return word2idx[\"unk\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9byefh6DSjV"
      },
      "source": [
        "dataset = dataset.map(\n",
        "    lambda item: {\n",
        "        \"features\": [encode(word) for word in item[\"tokenized\"]]\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "dIHzqnd_6AQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiHvDSTAHJ8D"
      },
      "source": [
        "dataset = dataset.remove_columns([\"text\", \"tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOI1-AlYlbgA"
      },
      "source": [
        "Переведем в тензоры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZ9EBnCK9V3"
      },
      "source": [
        "dataset.set_format(type='torch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWdVJKsnPtYX"
      },
      "source": [
        "dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6haisMrlmSu"
      },
      "source": [
        "Хотим склеить объекты разной длинны в батчи. Для этого давайте напишем `collate_fn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzwuahwBxb2l"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    max_len = max(len(row[\"features\"]) for row in batch)\n",
        "    input_embeds = torch.empty((len(batch), max_len), dtype=torch.long)\n",
        "    labels = torch.empty(len(batch), dtype=torch.long)\n",
        "    for idx, row in enumerate(batch):\n",
        "        to_pad = max_len - len(row[\"features\"])\n",
        "        input_embeds[idx] = torch.cat((row[\"features\"], torch.zeros(to_pad)))\n",
        "        labels[idx] = row[\"label\"]\n",
        "    return {\"features\": input_embeds, \"labels\": labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.items())\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "q4vwCHW88Gog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdVbtWJzszbu"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loaders = {\n",
        "    k: DataLoader(\n",
        "        ds, shuffle=(k==\"train\"), batch_size=32, collate_fn=collate_fn\n",
        "    ) for k, ds in dataset.items()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaders"
      ],
      "metadata": {
        "id": "aXxNYaQcHBFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsqpZ-WfPqe"
      },
      "source": [
        "## CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9DMopWCiBlD"
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(len(word2idx), embedding_dim=embed_size)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(embed_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.cl = nn.Sequential(\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)  # (batch_size, seq_len, embed_dim)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.cnn(x)\n",
        "        prediction = self.cl(x)\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "_lmIg1tVLSvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ed-3nQHjDrd"
      },
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CNNModel(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oZjYEBkgauj"
      },
      "source": [
        "Подготовим функцию для обучения модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Smd5_03CSp"
      },
      "source": [
        "\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "\n",
        "def training(model, criterion, optimizer, num_epochs, loaders, max_grad_norm=2):\n",
        "    for e in trange(num_epochs, leave=False):\n",
        "        model.train()\n",
        "        num_iter = 0\n",
        "        pbar = tqdm(loaders[\"train\"], leave=False)\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "            input_embeds = batch[\"features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            prediction = model(input_embeds)\n",
        "            loss = criterion(prediction, labels)\n",
        "            loss.backward()\n",
        "            if max_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            num_iter += 1\n",
        "        valid_loss = 0\n",
        "        valid_acc = 0\n",
        "        num_iter = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            num_objs = 0\n",
        "            for batch in loaders[\"test\"]:\n",
        "                input_embeds = batch[\"features\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                prediction = model(input_embeds)\n",
        "                valid_loss += criterion(prediction, labels)\n",
        "                correct += (labels == prediction.argmax(-1)).float().sum()\n",
        "                num_objs += len(labels)\n",
        "                num_iter += 1\n",
        "        \n",
        "\n",
        "        print(f\"Test valid Loss: {valid_loss / num_iter}, test accuracy: {correct/num_objs}\")\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            num_objs = 0\n",
        "            for batch in loaders[\"train\"]:\n",
        "                input_embeds = batch[\"features\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                prediction = model(input_embeds)\n",
        "                valid_loss += criterion(prediction, labels)\n",
        "                correct += (labels == prediction.argmax(-1)).float().sum()\n",
        "                num_objs += len(labels)\n",
        "                num_iter += 1\n",
        "        \n",
        "\n",
        "        print(f\"Train valid Loss: {valid_loss / num_iter},train accuracy: {correct/num_objs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "LHoDkBoMOA5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOi77V2XjDpg"
      },
      "source": [
        "%time training(model, criterion, optimizer, num_epochs, loaders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN8E8-ZSgnul"
      },
      "source": [
        "## RNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(len(word2idx), embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, num_layers=2)\n",
        "        self.linear = nn.Linear(hidden_dim, 6)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        rnn_out, ht = self.rnn(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "metadata": {
        "id": "YuzCCQfb7su5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = RNN_fixed_len(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1.0"
      ],
      "metadata": {
        "id": "vmQ2e-228h3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx62ofTU3CNY"
      },
      "source": [
        "%time training(model, criterion, optimizer, num_epochs, loaders, max_grad_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJqh9eWIhxE0"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(len(word2idx), embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=4)\n",
        "        self.linear = nn.Linear(hidden_dim, 6)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "metadata": {
        "id": "56uU77Z87smr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LSTM_fixed_len(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1.0"
      ],
      "metadata": {
        "id": "JItaXLah7so6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time training(model, criterion, optimizer, num_epochs, loaders)"
      ],
      "metadata": {
        "id": "hqmC261E7srR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QRNN"
      ],
      "metadata": {
        "id": "TC2_gP3iOtll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvrtc git+https://github.com/salesforce/pytorch-qrnn"
      ],
      "metadata": {
        "id": "9m2lxynxZqOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDRH3TkK6VS"
      },
      "source": [
        "import cupy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pynvrtc"
      ],
      "metadata": {
        "id": "59Jvo_CkiTRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchqrnn"
      ],
      "metadata": {
        "id": "-Fq2cZnyaA5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchqrnn import QRNN"
      ],
      "metadata": {
        "id": "KKP1hNPXjJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, hidden_size, num_classes=6, parallel=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(word2idx), embed_size)\n",
        "        self.rnn = QRNN(embed_size, hidden_size, num_layers=4)\n",
        "        #self.rnn = nn.LSTM(hidden_size, hidden_size)\n",
        "        # Note: we tell DataParallel to split on the second dimension as RNNs are batch second by default in PyTorch\n",
        "        if parallel: self.rnn = nn.DataParallel(self.rnn, dim=1)\n",
        "        self.cls = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.rnn(x)\n",
        "        qrnn_out, ht= self.cls(x)\n",
        "        ht.permute(1, 0, 2)\n",
        "        return self.cls(ht[-1])\n"
      ],
      "metadata": {
        "id": "d1hpJLhDfzPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Model(word2vec.vector_size, 50).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "num_epochs = 4\n",
        "max_grad_norm = 2.0"
      ],
      "metadata": {
        "id": "neig3SjqkFBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time training(model, criterion, optimizer, num_epochs, loaders, max_grad_norm)"
      ],
      "metadata": {
        "id": "l2fRQkIvnOT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "К сожалению, проверить архитектуру QRNN не удалось, предположительно из-за устаревшей библиотеки, предлагаемой авторами статьи arXiv:1611.01576v2 [cs.NE] 21 Nov 2016 James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher."
      ],
      "metadata": {
        "id": "nWLpy7DhOwWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKEFrwLfPcdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}